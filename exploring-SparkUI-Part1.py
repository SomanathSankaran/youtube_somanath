# Databricks notebook source
# MAGIC %md
# MAGIC #Overview

# COMMAND ----------

# MAGIC %md
# MAGIC Understanding **Spark UI** is critical and it is first step in debugging or fine tuning  the Spark Application  
# MAGIC As part of this Session ,We will understand  
# MAGIC - Databricsk Cluster UI -various tabs
# MAGIC - Spark SQL UI
# MAGIC - Explain Command 
# MAGIC While Selecting and filtering Data from a **Parquet File**

# COMMAND ----------

# MAGIC %md
# MAGIC Here's a tabular breakdown of the different tabs in the Cluster configuration:
# MAGIC
# MAGIC | Tab                  | Description                                                                                           |
# MAGIC |----------------------|-------------------------------------------------------------------------------------------------------|
# MAGIC | Configuration        | Adjust settings related to cluster behavior, resources, and environment.                            |
# MAGIC | Notebooks        | Access and interact with notebooks for code development and analysis.          |
# MAGIC | Libraries            | Install and manage additional libraries and packages for cluster usage.                             |
# MAGIC | Event log            | Monitor and analyze cluster events and activities for troubleshooting and performance optimization cluster start event end autoscaling GC issue. |
# MAGIC | Spark UI             | Access the web-based interface to monitor Spark applications, jobs, and tasks.                     |
# MAGIC | Driver logs          | Review logs generated by the driver program for diagnosing issues and tracking execution details like GC and other issues.   |
# MAGIC | Metrics              | View and analyze performance metrics and statistics for resource usage metrics           |
# MAGIC | Apps                 | Contains other apps like web terminal and R Studio on driver node  |
# MAGIC | Spark compute UI - Master | Access the UI for monitoring and managing the master node's computational resources.              |
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC | **Tab Name**       | **Description**                                                                                          |
# MAGIC |--------------------|----------------------------------------------------------------------------------------------------------|
# MAGIC | **Jobs**           | Displays information about the completed and running jobs, including their stages, tasks, and execution details. |
# MAGIC | **Stages**         | Shows information about the stages of a job, including their status, number of tasks, and execution time.  |
# MAGIC | **Storage**        | Provides details on RDDs and DataFrames currently stored in memory and their storage levels including delta cache and metadata cache.             |
# MAGIC | **Executors**      | Displays information about the Spark executors, including memory usage, task counts, and disk usage.      |
# MAGIC | **SQL**            | Shows information about the SQL queries that have been executed, including the query plans and execution metrics. |
# MAGIC | **Environment**    | Displays the environment variables, Spark configuration, and system properties for the current application. |
# MAGIC |**JDBC ODBC Server**| Containing queries executing through JDBC /ODBC such as DBT tool Applications|
# MAGIC |**Connect**| Containing queries executed through Databricks Connect|

# COMMAND ----------

# MAGIC %md
# MAGIC #Load Sample Data

# COMMAND ----------

# MAGIC %md
# MAGIC Data is loaded using azure open Dataset
# MAGIC [url](https://learn.microsoft.com/en-us/azure/open-datasets/dataset-taxi-yellow?tabs=pyspark#volume-and-retention)

# COMMAND ----------

  def load_data():
    # Azure storage access info
    blob_account_name = "azureopendatastorage"
    blob_container_name = "nyctlc"
    blob_relative_path = "yellow"
    blob_sas_token = "r"

    # Allow SPARK to read from Blob remotely
    wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)
    spark.conf.set(
      'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),
      blob_sas_token)
    print('Remote blob path: ' + wasbs_path)

    # SPARK read parquet, note that it won't load any data yet by now
    df = spark.read.parquet(wasbs_path)
    
    return df

# COMMAND ----------

df=load_data()

# COMMAND ----------

df.limit(5).display()

# COMMAND ----------

# MAGIC %md
# MAGIC #Exploring Folder Hierarchy

# COMMAND ----------

display(dbutils.fs.ls("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow"))

# COMMAND ----------

display(dbutils.fs.ls("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/"))

# COMMAND ----------

# MAGIC
# MAGIC    %md
# MAGIC Explain  has below signature  
# MAGIC     ` def explain(
# MAGIC         self, extended: Optional[Union[bool, str]] = None, mode: Optional[str] = None
# MAGIC     ) -> None:
# MAGIC    `
# MAGIC [source](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html)
# MAGIC

# COMMAND ----------

df.limit(10).explain()

# COMMAND ----------

df.limit(10).explain(extended=True)

# COMMAND ----------

df.limit(10).explain(mode="codegen")

# COMMAND ----------

df.limit(10).explain(mode="formatted")

# COMMAND ----------

# MAGIC %md
# MAGIC # Exploring Spark SQL UI

# COMMAND ----------

df.count()

# COMMAND ----------

df.where("puYear=2012").selectExpr("_metadata.file_path","_metadata.file_size","*").display()

# COMMAND ----------

df.where("puYear=2012").selectExpr("_metadata.file_path").distinct().display()

# COMMAND ----------

df.where("puYear=2012 and puMonth=3").display()

# COMMAND ----------

# MAGIC %md
# MAGIC #Enabling Disk IO Cache

# COMMAND ----------

spark.conf.set("spark.databricks.io.cache.enabled", "true")


# COMMAND ----------

df.where("puYear=2012 and puMonth=3").display()

# COMMAND ----------

df.where("puYear=2012 and puMonth=4").display()

# COMMAND ----------

df.where("puYear=2012 and puMonth=4").display()

# COMMAND ----------

df.where("puYear=2012 and puMonth=4 and vendorID='CMT'").select("_metadata.file_path","*").limit(10).display()

# COMMAND ----------

# MAGIC %md
# MAGIC #Understand Parquet Internal

# COMMAND ----------

# MAGIC %md
# MAGIC Read more about parquet here
# MAGIC [src](https://www.gresearch.com/news/parquet-files-know-your-scaling-limits/)

# COMMAND ----------

# MAGIC %md
# MAGIC ![img](https://www.gresearch.com/wp-content/uploads/2023/07/Spark-Parquet-inspection-viz-1.webp)

# COMMAND ----------

import gresearch.spark.parquet

# COMMAND ----------

spark.read.parquet_metadata("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/puMonth=4/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-45.c000.snappy.parquet").display()


# COMMAND ----------

spark.read.parquet_blocks("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/puMonth=4/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-45.c000.snappy.parquet").display()


# COMMAND ----------

spark.read.parquet_block_columns("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/puMonth=4/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-45.c000.snappy.parquet").display()


# COMMAND ----------

spark.read.parquet_block_columns("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/puMonth=4/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-45.c000.snappy.parquet").where("array_contains(column,'vendorID')").display()


# COMMAND ----------

#["passengerCount"]

spark.read.parquet_block_columns("wasbs://nyctlc@azureopendatastorage.blob.core.windows.net/yellow/puYear=2012/puMonth=4/part-00000-tid-8898858832658823408-a1de80bd-eed3-4d11-b9d4-fa74bfbd47bc-426339-45.c000.snappy.parquet").where("array_contains(column,'passengerCount')").display()


# COMMAND ----------

df.where("puYear=2012 and puMonth=4 and passengerCount=9").select("_metadata.file_path","*").limit(10).display()

# COMMAND ----------

# MAGIC %md 
# MAGIC #Conclusion
# MAGIC
# MAGIC - Parquet stores data in columnar format 
# MAGIC - We cant have full columnar as we need to seleect other columns as well.
# MAGIC - Hence it stores as a group of rows with all column value stats stored
# MAGIC -  parquet despite adding parquet filter and predicate pushdown
# MAGIC - we need to layout the data in better way by understanding query patterns  and
# MAGIC - optimizing the layout by techniques like **z-order** 

# COMMAND ----------

