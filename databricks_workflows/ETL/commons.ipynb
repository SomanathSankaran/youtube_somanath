{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea432d4-cda5-43ba-9cea-e624f89a2167",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, current_user,expr\n",
    "\n",
    "def load_and_append_dedup(\n",
    "    source_table: str,\n",
    "    target_table: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads data from a source table, deduplicates rows based on specified columns,\n",
    "    adds metadata columns ('ingested_timestamp' and 'ingested_by'), and appends the result to a target table.\n",
    "\n",
    "    Args:\n",
    "        source_table (str): Name of the source table to read from.\n",
    "        target_table (str): Name of the target table to append to.\n",
    "        dedup_columns (list): List of column names to use for deduplication.\n",
    "        ingested_by (str): Identifier for the user or process performing the ingestion.\n",
    "    \"\"\"\n",
    "    # Read source table\n",
    "    if source_table.endswith(\"lineitem\"):\n",
    "        src_df = spark.table(source_table)\n",
    "        #src_df = spark.table(source_table+\"_v1\") # Simulating failure in for loop\n",
    "    else:\n",
    "        src_df = spark.table(source_table)\n",
    "    \n",
    "    # Deduplicate rows based on dedup_columns\n",
    "    deduped_df = src_df.dropDuplicates()\n",
    "    \n",
    "    # Add metadata columns\n",
    "    result_df = deduped_df.withColumn(\"ingested_timestamp\", current_timestamp()) \\\n",
    "                          .withColumn(\"ingested_by\", current_user())\n",
    "    \n",
    "    # Append to target table\n",
    "    result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5bcd1b6-9ce3-4acb-9904-2d7f52f080ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def insert_audit_record(\n",
    "    usecase_id,\n",
    "    workspace_url,\n",
    "    workspace_id,\n",
    "    job_params,\n",
    "    start_time,\n",
    "    triggered_time,\n",
    "    status\n",
    "):\n",
    "    \"\"\"\n",
    "    Inserts an audit record into the workflow_meta.audit_table with metadata and timing information.\n",
    "\n",
    "    Args:\n",
    "        workspace_url (str): URL of the workspace.\n",
    "        job_params (str): Job parameters.\n",
    "        start_time (str): Time when the job started (ISO format).\n",
    "        triggered_time (str): Time when the job was triggered (ISO format).\n",
    "        status (str): Status of the workflow run.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        spark.createDataFrame(\n",
    "            [[usecase_id,workspace_url,workspace_id, job_params, start_time, triggered_time, status]],\n",
    "            \"usecase_id  string,workspace_url string,workspaceid string, job_params string, start_time string, triggered_time string, status string\"\n",
    "        ))\n",
    "    df=(df.withColumn(\"job_run_url\", expr(\"concat_ws('/',split(workspace_url,r'\\\\?')[0],job_params)\"))\n",
    "        .withColumn(\"start_time\", expr(\"to_timestamp(start_time)\"))\n",
    "        .withColumn(\"trigger_time\", expr(\"to_timestamp(triggered_time)\"))\n",
    "        .selectExpr(\n",
    "            \"*\",\n",
    "            \"cast(timestampdiff(MILLISECOND, triggered_time, start_time) as double) waiting_time_in_ms\",\n",
    "            \"cast(timestampdiff(SECOND, start_time, current_timestamp()) as double) as total_time\",\n",
    "            \"current_timestamp() as ingested_timestamp\",\n",
    "            \"current_user() as ingested_by\"\n",
    "        )\n",
    "    )\n",
    "    df.select(spark.table(\"workflow_meta.audit_table\").columns).write.format(\"delta\").mode(\"append\").saveAsTable(\"workflow_meta.audit_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "commons",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
