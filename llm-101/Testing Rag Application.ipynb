{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9091368-7248-4e76-93e0-79c6f7d6b41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Primer: LangChain & Simplifying LLM Apps\n",
    "\n",
    "**LangChain** is a powerful framework for building applications with large language models (LLMs). It provides:\n",
    "\n",
    "- **Integrations**: Connects LLMs (OpenAI, HuggingFace, Databricks, etc.) with data sources, APIs, and tools.\n",
    "- **Chains**: Orchestrates sequences of LLM calls and logic, making complex workflows easy to build.\n",
    "- **Retrieval-Augmented Generation (RAG)**: Combines LLMs with external knowledge (e.g., documents, databases) for more accurate, context-aware responses.\n",
    "- **Vector Search**: Enables fast similarity search over embedded data, crucial for RAG.\n",
    "\n",
    "### Why LangChain for LLM Apps?\n",
    "\n",
    "- **Rapid Prototyping**: Prebuilt components for prompts, memory, agents, and retrieval.\n",
    "- **Extensibility**: Easily plug in new models, data sources, or tools.\n",
    "- **Production-Ready**: Integrates with MLflow for experiment tracking and Databricks for scalable deployment.\n",
    "\n",
    "### Databricks Vector Search + LangChain\n",
    "\n",
    "- Store and index document embeddings in Databricks.\n",
    "- Use LangChain to retrieve relevant context for user queries via vector search.\n",
    "- Pass retrieved context to LLMs for accurate, up-to-date answers.\n",
    "\n",
    "**Example Workflow:**\n",
    "1. Ingest and embed documents.\n",
    "2. Store embeddings in Databricks Vector Search.\n",
    "3. On user query, retrieve similar documents.\n",
    "4. Use LLM (via LangChain) to generate a response using retrieved context.\n",
    "\n",
    "LangChain abstracts the complexity, letting you focus on building intelligent, data-aware LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86fa30ca-e476-4531-8e74-278270ddd1d9",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1759470163654}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"op\":\"OR\",\"filterGroupId\":\"fg_834d15f1\",\"filters\":[{\"filterId\":\"f_46b4a672\",\"columnId\":\"url\",\"enabled\":true,\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1759470170431},{\"enabled\":true,\"filterGroupId\":\"fg_cb63758e\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_40228ec4\",\"enabled\":true,\"columnId\":\"url\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"https://docs.databricks.com/en/admin/disaster-recovery.html\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1759553752123}],\"syncTimestamp\":1759553752123}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"and\",\"args\":[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"url\"},{\"kind\":\"literal\",\"value\":\"https://docs.databricks.com/en/admin/disaster-recovery.html\",\"type\":\"string\"}]}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.llm_rag.rag_input_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271e7873-6b36-4821-baf1-2a688a4aeba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct url from workspace.llm_rag.rag_input_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab7661b-53e6-42e6-ad6c-92970fdc9d12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade databricks-langchain langchain-community langchain databricks-sql-connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e917c1-a703-4844-bd96-785c62c56b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b0569e-d875-430b-bb6d-5e5d0e8c7322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Call Langchain without RAG Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb39034-2510-45e9-bd1f-25bae9c3320e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-405b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=250,\n",
    ")\n",
    "resp=chat_model.invoke(\"provide step-by-step process to do disaster recovery in  Databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9496791e-36c1-4128-8033-e5a5bd4eb925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847e70ac-7d8b-4b3e-8750-64277a4ca3d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "vector_store = DatabricksVectorSearch(index_name=\"workspace.llm_rag.databricks_vector_index\")\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "response=retriever.invoke(\"provide step-by-step process to do disaster recovery in  Databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf69085d-614f-4c18-bd32-3f4a876a253e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(response[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba76fba2-ae28-4a6b-be0a-5cb4730c1a8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Using RAG Additional context to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b5dc12-27a5-49c1-90c4-8b10b37acbc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define the prompt template that includes the context\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of context to answer the user's question. \"\n",
    "    \"If you don't know the answer based *only* on the context, say that you don't know.\"\n",
    "    \"\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "curated_response=chat_model.invoke(prompt.format_messages(input=\"provide step-by-step process to do disaster recovery in  Databricks\", context=response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa16884f-9f8c-41d5-810d-79e4f66f41e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(curated_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5216d9c9-4919-4f00-954a-0548630c9f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Log LLM Experiments with MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70865a58-d7c3-4026-842b-438a583d7d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c4de41c-8d4d-458d-a5e8-204f67890bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Going Beyond Summary Providing Full information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f6f4ac0-6bc1-41d3-8e78-d47f54671f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks_langchain import DatabricksVectorSearch\n",
    "\n",
    "vector_store = DatabricksVectorSearch(index_name=\"workspace.llm_rag.databricks_vector_index\",columns=[\"url\",\"content\"],)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "response=retriever.invoke(\"provide step-by-step process to do disaster recovery in  Databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c6a4e8-ec92-492d-b97d-d852c1a1edce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd06fa4-f4b0-4297-b2a9-b99021162248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_docs=[[r.metadata[\"url\"],r.metadata[\"content\"]] for r in response]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beffae2f-4172-4916-a4bb-4a756c52f186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af955ebd-e3d8-49fa-88c1-7e1e1d265161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define the prompt template that includes the context\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of context to answer the user's question. \"\n",
    "    \"If you don't know the answer based *only* on the context, say that you don't know.\"\n",
    "    \"\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "curated_response=chat_model.invoke(prompt.format_messages(input=\"provide step-by-step process to do disaster recovery in  Databricks\", context=combined_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d108d7b7-2a1c-4a65-b448-0a49ec93c2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "chat_model = ChatDatabricks(\n",
    "    endpoint=\"databricks-meta-llama-3-1-405b-instruct\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=4096,\n",
    ")\n",
    "resp=chat_model.invoke(\"provide step-by-step process to do disaster recovery in  Databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c14bd2ed-03dc-4876-ba58-7424b8e8ba16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# 2. Define the prompt template that includes the context\n",
    "system_prompt = (\n",
    "    \"Use the following pieces of context to answer the user's question. \"\n",
    "    \"If you don't know the answer based *only* on the context, say that you don't know.\"\n",
    "    \"\\n\\nContext: {context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "curated_response=chat_model.invoke(prompt.format_messages(input=\"provide step-by-step process to do disaster recovery in  Databricks\", context=combined_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f843a8ae-888b-43d4-b8e3-aa7dba2e11b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6453657712253485,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Testing Rag Application",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
